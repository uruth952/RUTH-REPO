---
title: "FINAL EXAM FOR POLI 210"
author: "RUTH UZOKA"
date: "2023-12-09"
output: pdf_document
---
##Questions-
1)We have a variable that lists income in units of 'thousands of dollars' (1, 2, 3, etc) but we recode it into actual dollars. That is, we multiply our original variable by 1000.

a) Will this change the mean of the variable?
  
b)Will this change the variance of the variable?
  
c)Create an example of this in R, and show the result from part a and b above in a sensible way.

###ANSWER
a-The income in units of measurement being changed from 'thousands of dollar' to actual dollars will change the mean of the variable by a 1000.

b-The variance of the variable will definitely change because a change in the unit of measurement will cause a change in the  variance of the variable. Moreso, variance being a measure of squared deviations will multiply the variable by 1000^2 and hence it will result to a much greater or larger variance.

c-

```{r}
####variable that lists 
income_units_thousand_dollars <- c(1,2,3,4, 5)

actual_dollar_income <- income_units_thousand_dollars * 1000

##calculate the mean
mean_thd_dollars<-mean(income_units_thousand_dollars)
mean_dollars<-mean(actual_dollar_income)
mean_thd_dollars
mean_dollars
##calculate the variance
var_thd <- var(income_units_thousand_dollars)
var <- var(actual_dollar_income)
var_thd
var

```
2)  Imagine you run an experiment where you randomly assign individuals to receive one of two mailers. Call one mailer the *treatment*, and the other mailer *control*. Your dependent variable is turnout. Of the 1000 people sent the treatment mailer, 380 turnout in the next election. Of the 1000 people sent the control mailer, 302 turnout in the next election.

 a)  What is your best estimate of difference in turnout rates between the two groups?

 b)  Estimate the standard error of this difference using R. Show your calculations.

c)  Estimate a:

        i)  90% confidence interval for the difference

        ii) 95% confidence interval for the difference
d)  Say we pick an alpha of 0.05, can we reject the null hypothesis that there is no difference in turnout between the two groups?

##ANSWER-
a)My best estimate of difference in turnout between the treatment and control group- That is T(380)/1000 -C(302)/1000

0.38-0.302 = 0.078 or 7.8%

b)Given that the turnout for treatment group is =380 0r 0.38
Turnout for Control group is =302 or 0.302

Therefore; The standard error will be 0.38*(1-0.38)/ 1000 +0.302*(1- 0.302) / 1000)
=0.0002356 + 0.00210796
=(0.00446196)###The square root of this gives
= 0.0211

To show this in R
```{r }
t=0.38##T represent Treatment,C-control,SE-standard error
##Number of sample size =1000
c = 0.302

##Therefore,
treatment_group <- 0.38
control_group <- 0.302
number_sample_size <-1000

standarderror_of_treatment_group <- sqrt(treatment_group *(1-treatment_group)/number_sample_size)### for treatment group

standarderror_control_group <- sqrt(treatment_group *(1-control_group)/number_sample_size)###for control group

standarderror <-sqrt((treatment_group*(1-treatment_group)/number_sample_size) +(control_group *(1-control_group)/number_sample_size))###for both groups

standarderror
```
```{r }
t=0.38 ##T represent Treatment,C-control,SE-standard error
##Number of sample size =1000
c = 0.302

##Therefore,
treatment_group<-0.38
control_group<-0.302
number_sample_size<-1000

standarderror_treatment_group <- sqrt(treatment_group *(1-treatment_group)/number_sample_size)### for treatment group

standarderror_control_group <- sqrt(treatment_group *(1-control_group)/number_sample_size)###for control group

standarderror <-sqrt((treatment_group*(1-treatment_group)/number_sample_size) +(control_group *(1-control_group)/number_sample_size))###for both groups

####To estimate a 90% confidence interval for the difference-

dce_90 <- qt(0.95,  df= number_sample_size-2) * standarderror

dci_90 <- c(treatment_group - control_group - dce_90, treatment_group - control_group + dce_90)

###for 95%
dce_95 <- qt(0.975, df= number_sample_size-2) * standarderror

dci_95 <- c(treatment_group - control_group -dce_95,treatment_group - control_group + dce_95)


dci_90
dci_95

# t-test
(treatment_group - control_group) / standarderror

```
d)From the outcome we got,the t-score is greater than the critical value of 1.96,Therefore,there is a significant statistical difference between the two groups that received the mailers, therefore with an alpha of 0.05, we reject the null hypothesis.


3)  According to the Pew Research Center for the People and the Press, in 2003, the contact rate for surveyors was 76%. That is 76% of the numbers they try to reach (which have been generated by random digit dialers) are successfully reached. Assume that any one household's response is independent of all others. Answer the following questions. You may do so using calculation or simulation. Show your code/work as much as possible.

    a)  What is the probability that an interviewer will contact the next household on her list?

    b)  What is the probability that the interviewer will contact the next two households on her list?

    c)  What is the probability that the interviewer will make at least one successful contact among the next five households on her list?
    
####ANSWER
a)The probability that an interviewer will contact the next household on her list is-P=number of outcomes that are successful/ total number of outcomes that are possible. hence-P=76/100 = 0.76 or 76%


b)The probability that the interviewer will contact the next two households on her list is =P(first household)*p(second household)= 0.76 * 0.76 =0.58 approximately.

c)The probability that the interviewer will make at least one successful contact among the next five households on her list = (1 - 0.76) = 0.24 * 0.24 *0.24 * 0.24 * 0.24= 0.000796

hence-(1-0.000796) =0.999 or 99%




4)  We have data from 10 city council elections recording the vote share of each candidate. We are interested to know whether or not incumbents have a significant vote advantage at the local level.

| Incumbent | Challenger |
|-----------|------------|
| .359      | 0.641      |
| .511      | 0.489      |
| .467      | 0.533      |
| .643      | 0.357      |
| .521      | 0.479      |
| .671      | 0.329      |
| .491      | 0.509      |
| .228      | 0.772      |
| .905      | 0.095      |
| .503      | 0.497      |

a)  State the null and research hypotheses.

b)  Run an appropriate test in R and paste your results.

c)  What is the p-value for your test statistic?

d)  State your conclusions.

#####ANSWER-
a)The null hypothesis states that there is an equal statistical vote share between incumbent and the challenger.

The research hypothesis states that the vote share between the incumbent and challenger is not equal.

b)check chunk for test-

```{r}
#from the data;
incumbent <-c(0.359,0.511,0.467,0.643,0.521,0.671,0.491,0.228,0.905,0.503)
challenger <-c(0.641,0.489,0.533,0.357,0.479,0.329,0.509,0.772,0.095,0.497)

outcome <- t.test(incumbent, challenger, paired=TRUE)
outcome

```
c)We accept the null hypothesis since the P value which is 0.61 is greater than the significant value which is 0.05.

```{r}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(grDevices)
library(grid)
library(broom)
library(tidyverse)
library(tidyr)


```
# Read in cq_data.csv file savide it as a variable cq_data

# This is based on a paper that was published at Research and Politics.
# The point of this part of the final is to see if you can replicate a published paper
# with the skills you've learned this semester.
# Some parts of the code are done for you. Try to still see if you can figure out what they do.
# It might help to read up on the dplyr package

# The data has dv which is the Democratic party's vote share for districts i in year t
# there is also a variable called dvp, which is the Democratic party's vote share for districts i in year t-1 (the previous year)

1 write a function called nationalization that takes two vectors called "dv" and "dvp, and returns a single number that corresponds to the measure "nationalization" in the paper by LeVeck and Nail. Make sure to handle NA's. You don't have to worry about the fact that the measure is computed for a specific year, that's handled later. You can just assume all dv and dvp observations come from one year.



```{r}
gary_j_data <- read.csv("gary_j_data.csv")
cq_data <- read.csv("cq_data.csv")

nationalization <- function(dv, dvp){
  df<-(dv - dvp)
  nationalization<- -sd(df)
  return(nationalization)
}
nationalization(cq_data$dv,cq_data$dvp)



```
# 2
# Calculate nationalization measure by year
# requires dplyr
# This should work if you've properly programmed the functions abov
# Calculate correlation between Gary J' measure and our measure of nationalization
# load gary j's data 

# Make a dataset with both our measure and 
# gary J's measure of nationalization for every year.

```{r}
swing_variance <- cq_data %>%
  filter(inc3 !=0) %>%
  group_by(year) %>%
  summarise(nationalization = nationalization(dv, dvp)
  ) %>%
  ungroup()

measure_comparison <- gary_j_data %>%
  filter(year>=1954) %>%
  group_by(year) %>%
  summarise(nationalization1 = nationalization(dv, dvp), # our measure
            nationalization2 = cor(dv,dpres,use = "pairwise.complete.obs") # garyj's measure
            )
```

# 3-compare the pearson correlation between our measure and gary j's nationalization measure


####################################################
# Calculate Incumbency advantage  coeffients by year
# Join these estimates with our nationalization measure
#####################################################

```{r}
b_incumbency_adv <- cq_data %>%
  group_by(year) %>% 
  do(., tidy(lm(dv ~ dvp + ptynow + inc3, data = . ))) %>% 
  filter(term == "inc3") %>%
  ungroup() %>% 
  left_join(., swing_variance, by="year") %>% # join to nationalization measure
  mutate(post_1952 = as.numeric(year>1952)) # dummy for whether the year is after
# 1952
# Add in markers for 1950 and 1954
# This will be used in figure 1c
b_incumbency_adv$marker_break <- NA
b_incumbency_adv$marker_break[b_incumbency_adv$year==1950] <- 1950
b_incumbency_adv$marker_break[b_incumbency_adv$year==1954] <- 1954

```
# plot nationalization over time 
# this is all done for you, but you can try to learn about
# making graphics from this code
# might be useful in the future?
```{r}

nationalization_year_plot <- ggplot(data=swing_variance, 
                                    aes(x= year, 
                                        y= nationalization)) +
  geom_line() +
  geom_point() +
  theme_bw() + ylab("Nationalization") + xlab("Year")


nationalization_year_plot

```
# 4 run the line below to make sure that you get a similar graph to 1a
# it won't have standard error bars like the main paper. 5pts extra credit if you can get those
# by bootstrapping your nationalization measure in every year


```{r}
# plot incumbency advantage over time
incumbency_adv_year_plot <- ggplot(data=b_incumbency_adv, 
                                   aes(x=year, y=estimate)) +
  geom_line() +
  geom_point() +
  theme_bw() + ylab("Incumbency Advantage") + xlab("Year")

incumbency_adv_year_plot
```
# 5 run the line below to make sure that you get a similar graph to 1b
```{r}
incumbency_adv_year_plot

```


# 6a Find the pearson correlation between nationalization and the incumbency advantage by era (pre 1952 and post 1952) using the b_incumbency_adv data


# 6b Find the spearman correlation between nationalization and the incumbency advantage by era (pre 1952 and post 1952) using the b_incumbency_adv data


# Plot relationship between nationalization and the incumbency advantage

```{r}
###For pearson pre era
pre_1952 <- b_incumbency_adv[b_incumbency_adv$year < 1952,]
pearson_sum_pre_1952<-cor.test(pre_1952$nationalization,pre_1952$estimate,method="pearson")

####For post era

post_1952 <- b_incumbency_adv[b_incumbency_adv$year >= 1952,]
pearson_sum_post_1952<-cor.test(post_1952$nationalization,post_1952$estimate,method="pearson")

print(pearson_sum_pre_1952)
print(pearson_sum_post_1952)

```





```{r}
###For spearman pre era
pre_1952 <- b_incumbency_adv[b_incumbency_adv$year < 1952,]
spearman_sum_pre_1952<-cor.test(pre_1952$nationalization,pre_1952$estimate,method="spearman")

####For post era

post_1952 <- b_incumbency_adv[b_incumbency_adv$year >= 1952,]
spearman_sum_post_1952<-cor.test(post_1952$nationalization,post_1952$estimate,method="spearman")

print(spearman_sum_pre_1952)
print(spearman_sum_post_1952)
```

```{r}
nationalization_incumbency_path_plot <- ggplot(data=b_incumbency_adv, 
                                               aes(x=nationalization, 
                                                   y=estimate))+ 
  geom_point(aes(colour= year)) +
  geom_path(aes(color=year))+
  geom_text(size=4.5, aes(color=year, label=marker_break, hjust=-.3, vjust=.2)) +
  geom_text(label= "Post 1952:",
            x=-11, y=10.7, size=4.5, fontface="bold") +
  geom_text(label= "Pre 1952:",
            x=-11.5, y=4.8, size=4.5,fontface="bold") +
  theme_classic() + 
  xlab("Nationalization") + 
  ylab("Incumbency Advantage") +
  scale_color_continuous(name="Year") +
  xlim(c(-13.5,-2.5)) +
  theme(legend.key.height = unit(1.5, "cm"),
        legend.title.align=0)
nationalization_incumbency_path_plot


```
# 7 run the line below to make sure you get a similar plot to panel 1c
# This will look slightly different than plot 1c, which is a scatter plot
# This is a path diagram, which connects points in temporal order


